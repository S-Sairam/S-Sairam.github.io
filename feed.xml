<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://s-sairam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://s-sairam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-25T16:01:58+00:00</updated><id>https://s-sairam.github.io/feed.xml</id><title type="html">blank</title><subtitle>Aspiring Causal &amp; Neuro-Symbolic AI Researcher</subtitle><entry><title type="html">The Geometry of Thought: Forging Structure from Chaos</title><link href="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/" rel="alternate" type="text/html" title="The Geometry of Thought: Forging Structure from Chaos"/><published>2025-10-25T00:00:00+00:00</published><updated>2025-10-25T00:00:00+00:00</updated><id>https://s-sairam.github.io/blog/2025/the-geometry-of-thought</id><content type="html" xml:base="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/"><![CDATA[<p>Why do the ‘thoughts’ of a neural network live in chaos?</p> <p>A standard deep learning model learns to represent data in a high-dimensional, unstructured latent space—a chaotic filing cabinet. It works, but it’s a black box. The flexibility comes at the cost of interpretability, mathematical consistency, and, potentially, robustness. This raises a fundamental question: can we force an AI to think in a more structured, geometric, and ultimately more stable way?</p> <p>This was the central question behind my first formal research project, <strong>BCMEM</strong>.</p> <p>Instead of looking to statistics for the answer, we looked to a deeper, more ancient source of structure: <strong>algebraic number theory.</strong> We hypothesized that the profound relationships governing quadratic forms, as described by Manjul Bhargava’s work on integer cubes, could be forged into a weapon—a new kind of <strong>architectural regularizer</strong> for neural networks.</p> <p>The mission was to build a system from first principles. We designed a novel, differentiable loss function that did not just guide the model toward a correct answer, but actively punished it for creating a mathematically inconsistent or chaotic internal representation of the world. We were teaching the machine not just <em>what</em> to think, but <em>how</em> its thoughts should be structured.</p> <p>The result was a victory. On the MNIST benchmark, our model achieved a competitive <strong>99.46% accuracy.</strong> But the real triumph was not the accuracy score. It was the emergence of order from chaos.</p> <p><img src="/assets/img/bhargava_cube_3d_visualization.png" alt="3D Embeddings"/></p> <p>As you can see, the latent space—the model’s “mind”—is no longer a chaotic cloud. It has been forced into a beautiful, interpretable crystal lattice, where each digit occupies its own clear, distinct region. This structure is not a happy accident; it is the <strong>direct, emergent consequence of the mathematical principles we imposed.</strong></p> <p>This project was never just about number theory. It was a successful test of a fundamental hypothesis that drives my entire research program: that the path to more <strong>robust, stable, and generalizable AI</strong> lies not just in bigger models, but in the principled injection of <strong>architectural regularization.</strong> We can, and we must, build machines that are not just powerful, but also possess an inherent, verifiable, and beautiful structure.</p> <p>This work was accepted for publication at the <strong>International Conference on Applied Algorithms (ICAA) 2026</strong> and will be published by Springer in their Lecture Notes in Computer Science.</p> <table> <tbody> <tr> <td><a href="https://github.com/S-Sairam/bcmem">View the Code on GitHub</a></td> <td><a href="link-to-your-arxiv">Read the Full Paper(Needs to be uploaded)</a></td> </tr> </tbody> </table>]]></content><author><name></name></author><category term="Robustness"/><category term="Representation Learning"/><category term="Inductive Bias"/><category term="Architectural Regularization"/><summary type="html"><![CDATA[A research project on using number theory to build more robust and interpretable AI systems.]]></summary></entry></feed>