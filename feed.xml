<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://s-sairam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://s-sairam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-03T00:38:37+00:00</updated><id>https://s-sairam.github.io/feed.xml</id><title type="html">blank</title><subtitle>Aspiring Causal &amp; Neuro-Symbolic AI Researcher</subtitle><entry><title type="html">The Geometry of Thought: An Early Exploration in Structured Representation</title><link href="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/" rel="alternate" type="text/html" title="The Geometry of Thought: An Early Exploration in Structured Representation"/><published>2025-10-29T00:00:00+00:00</published><updated>2025-10-29T00:00:00+00:00</updated><id>https://s-sairam.github.io/blog/2025/the-geometry-of-thought</id><content type="html" xml:base="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/"><![CDATA[<h3 id="can-we-force-a-model-to-think-in-a-more-structured-geometric-way">Can we force a model to think in a more structured, geometric way?</h3> <p>A standard deep learning model learns to represent data in a high-dimensional, unstructured latent space—a chaotic filing cabinet. It works, but it’s a black box. This raises a fundamental question: can we impose a deeper mathematical structure on a model’s internal “thoughts” to make them more interpretable and consistent?</p> <p>This question led to my first independent research project, undertaken out of my own interest in the intersection of pure mathematics and machine learning.</p> <p>Instead of looking to statistics for the answer, we turned to a deeper source of structure: <strong>algebraic number theory.</strong> We hypothesized that the relationships governing quadratic forms, as described by Manjul Bhargava’s work on integer cubes, could serve as a novel form of <strong>architectural regularization</strong> for a neural network.</p> <p>The goal was to build a system from first principles. We designed a differentiable loss function that operated independently of the main classification task. Its sole purpose was to penalize the model for creating mathematically inconsistent internal representations. We were testing if we could teach a machine not just <em>what</em> to think, but <em>how</em> its thoughts should be structured.</p> <p>The experiment yielded a clear result on the MNIST benchmark. While the model achieved a high accuracy of <strong>99.46%</strong>, the primary outcome was the structure of the latent space itself.</p> <p><img src="/assets/img/bhargava_cube_3d_visualization.png" alt="3D Embeddings"/></p> <p>As the visualization shows, the latent space is no longer an unstructured cloud. The embeddings have been organized into a distinct, clustered structure, a <strong>direct emergent consequence of the algebraic priors we imposed.</strong></p> <h3 id="the-key-lesson-from-novelty-to-rigor">The Key Lesson: From Novelty to Rigor</h3> <p>While this project achieved its proof-of-concept goals, its most valuable outcome was the lesson it taught me about the distinction between theoretical novelty and impactful research. It made me understand that a clever idea validated on a controlled, “toy” benchmark like MNIST is only the first step. True progress requires building systems that are not just elegant, but are also scalable and rigorously validated on complex, large-scale problems.</p> <p>This realization was the direct catalyst for my subsequent work. It now drives my research into <strong>[Artemis, a new class of optimizer]</strong> designed for superior performance on challenging, real-world tasks.</p> <p>This early study taught me that the goal isn’t just to build machines with beautiful internal structure, but to ensure that structure translates into verifiable gains in robustness and generalization where it matters most.</p> <p>To formalize this independent work, a paper was drafted and submitted for peer review. The process yielded invaluable feedback, highlighting several avenues for significant improvement—particularly the need for more rigorous, large-scale validation. Based on this expert critique, the paper was withdrawn to allow for the comprehensive rework it deserves.</p> <p><a href="https://github.com/S-Sairam/bcmem">View the Code on GitHub</a></p>]]></content><author><name></name></author><category term="Representation Learning"/><category term="Inductive Bias"/><category term="Architectural Regularization"/><category term="Undergraduate Research"/><summary type="html"><![CDATA[An independent undergraduate research project exploring the use of number-theoretic priors for structured representation learning.]]></summary></entry></feed>