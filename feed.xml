<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://s-sairam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://s-sairam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-18T11:56:18+00:00</updated><id>https://s-sairam.github.io/feed.xml</id><title type="html">blank</title><subtitle>Aspiring Causal &amp; Neuro-Symbolic AI Researcher</subtitle><entry><title type="html">Why Reproducibility is the Ultimate Teacher: A Clean-Room SAM Replication</title><link href="https://s-sairam.github.io/blog/2025/sam-replication/" rel="alternate" type="text/html" title="Why Reproducibility is the Ultimate Teacher: A Clean-Room SAM Replication"/><published>2025-11-03T00:00:00+00:00</published><updated>2025-11-03T00:00:00+00:00</updated><id>https://s-sairam.github.io/blog/2025/sam-replication</id><content type="html" xml:base="https://s-sairam.github.io/blog/2025/sam-replication/"><![CDATA[<h3 id="the-promise-of-deep-learning-is-often-ahead-of-our-understanding-to-bridge-that-gap-i-believe-in-deconstruction">The promise of deep learning is often ahead of our understanding. To bridge that gap, I believe in deconstruction.</h3> <p>A state-of-the-art paper presents an algorithm as a clean, two-page mathematical object. But its true character is only revealed in the chaotic, 200-epoch firefight of training. I chose the ICLR 2021 SAM paper not just to implement it, but to inhabit that space—to understand the core principles of sharpness and generalization from the ground up by watching them unfold.</p> <p>My goal was a <strong>clean-room replication</strong>: to build the optimizer based solely on the paper’s algorithmic description and force it to prove itself, with every step logged and publicly scrutinized. This transforms the exercise from mere implementation to genuine scientific validation.</p> <h3 id="the-process-and-the-struggle-a-tale-of-two-ascents">The Process and The Struggle: A Tale of Two Ascents</h3> <p>The logs tell a story of two distinct phases of learning. The initial ascent was a brute-force climb. The first epoch ended with a validation accuracy of just 53.17%, but the learning was explosive. In just 37 epochs, it had shattered the 90% accuracy barrier, a furious sprint through the easiest parts of the problem space.</p> <p>But the real struggle, the part that defines research, was the second ascent: the long, grueling climb from 90% to the frontier of the problem. This wasn’t a sprint; it was a siege. It took another <strong>104 epochs</strong> to crawl from 90.39% (Epoch 37) to break the 95% barrier (Epoch 141). This was a phase of meticulous, incremental refinement.</p> <h3 id="the-aha-moment-the-story-in-the-logs">The “Aha!” Moment: The Story in the Logs</h3> <p>The final <code class="language-plaintext highlighter-rouge">train_loss</code> was a near-perfect <code class="language-plaintext highlighter-rouge">0.0018</code>, but the true story was in the gap between training and validation. It became clear that SAM’s job isn’t to force <code class="language-plaintext highlighter-rouge">train_loss</code> to absolute zero, but to manage that gap. It’s a <strong>physical regularizer on the geometry of the solution space</strong>, sacrificing training perfection for a robust, “flat” basin.</p> <d-figure style="width: 100% !important; max-width: 680px !important; margin: 2rem auto !important; display: block !important;"> <img src="/assets/img/sam_wandb_graph.png" alt="W&amp;B Plot showing the long, steady climb of validation accuracy for the SAM optimizer" style="width: 100% !important; height: auto !important; border-radius: 8px;"/> <figcaption> The story of the run, logged on Weights &amp; Biases: validation accuracy's long, slow climb to its peak, mirroring the optimizer's search for a robust solution. </figcaption> </d-figure> <p>The final result, a test accuracy of <strong>96.74%</strong>, successfully validated the paper’s claims. But the real outcome was the hard-won intuition behind the numbers.</p> <h3 id="the-forward-look-from-deconstruction-to-architecture">The Forward Look: From Deconstruction to Architecture</h3> <p>This exercise in reproducibility wasn’t an end in itself. It was training. The patience learned during that 104-epoch climb is the same patience required to develop a new idea. This deep dive is the direct foundation for my current work on <strong>[Artemis, a novel optimizer]</strong>, which seeks to unify these geometric insights with the principled uncertainty of Bayesian inference. Because to build the future, you must first be able to perfectly, and patiently, rebuild the present.</p> <p><a href="https://github.com/S-Sairam/sam-optimizer">View the Code on GitHub</a></p> <p><a href="https://wandb.ai/pesu-ai-ml/sam-replication-cifar10/runs/mjyz5xy4">See the Full, Unabridged Training Logs on W&amp;B</a></p>]]></content><author><name></name></author><category term="Reproducibility"/><category term="Optimization"/><category term="Scientific Rigor"/><category term="Deep Learning"/><category term="Undergraduate Research"/><summary type="html"><![CDATA[A deep dive into scientific validation, deconstructing the ICLR 2021 SAM optimizer from first principles by analyzing the raw training process.]]></summary></entry><entry><title type="html">The Geometry of Thought: An Early Exploration in Structured Representation</title><link href="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/" rel="alternate" type="text/html" title="The Geometry of Thought: An Early Exploration in Structured Representation"/><published>2025-10-29T00:00:00+00:00</published><updated>2025-10-29T00:00:00+00:00</updated><id>https://s-sairam.github.io/blog/2025/the-geometry-of-thought</id><content type="html" xml:base="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/"><![CDATA[<h3 id="can-we-force-a-model-to-think-in-a-more-structured-geometric-way">Can we force a model to think in a more structured, geometric way?</h3> <p>A standard deep learning model learns to represent data in a high-dimensional, unstructured latent space—a chaotic filing cabinet. It works, but it’s a black box. This raises a fundamental question: can we impose a deeper mathematical structure on a model’s internal “thoughts” to make them more interpretable and consistent?</p> <p>This question led to my first independent research project, undertaken out of my own interest in the intersection of pure mathematics and machine learning.</p> <p>Instead of looking to statistics for the answer, we turned to a deeper source of structure: <strong>algebraic number theory.</strong> We hypothesized that the relationships governing quadratic forms, as described by Manjul Bhargava’s work on integer cubes, could serve as a novel form of <strong>architectural regularization</strong> for a neural network.</p> <p>The goal was to build a system from first principles. We designed a differentiable loss function that operated independently of the main classification task. Its sole purpose was to penalize the model for creating mathematically inconsistent internal representations. We were testing if we could teach a machine not just <em>what</em> to think, but <em>how</em> its thoughts should be structured.</p> <p>The experiment yielded a clear result on the MNIST benchmark. While the model achieved a high accuracy of <strong>99.46%</strong>, the primary outcome was the structure of the latent space itself.</p> <p><img src="/assets/img/bhargava_cube_3d_visualization.png" alt="3D Embeddings"/></p> <p>As the visualization shows, the latent space is no longer an unstructured cloud. The embeddings have been organized into a distinct, clustered structure, a <strong>direct emergent consequence of the algebraic priors we imposed.</strong></p> <h3 id="the-key-lesson-from-novelty-to-rigor">The Key Lesson: From Novelty to Rigor</h3> <p>While this project achieved its proof-of-concept goals, its most valuable outcome was the lesson it taught me about the distinction between theoretical novelty and impactful research. It made me understand that a clever idea validated on a controlled, “toy” benchmark like MNIST is only the first step. True progress requires building systems that are not just elegant, but are also scalable and rigorously validated on complex, large-scale problems.</p> <p>This realization was the direct catalyst for my subsequent work. It now drives my research into <strong>[Artemis, a new class of optimizer]</strong> designed for superior performance on challenging, real-world tasks.</p> <p>This early study taught me that the goal isn’t just to build machines with beautiful internal structure, but to ensure that structure translates into verifiable gains in robustness and generalization where it matters most.</p> <p>To formalize this independent work, a paper was drafted and submitted for peer review. The process yielded invaluable feedback, highlighting several avenues for significant improvement—particularly the need for more rigorous, large-scale validation. Based on this expert critique, the paper was withdrawn to allow for the comprehensive rework it deserves.</p> <p><a href="https://github.com/S-Sairam/bcmem">View the Code on GitHub</a></p>]]></content><author><name></name></author><category term="Representation Learning"/><category term="Inductive Bias"/><category term="Architectural Regularization"/><category term="Undergraduate Research"/><summary type="html"><![CDATA[An independent undergraduate research project exploring the use of number-theoretic priors for structured representation learning.]]></summary></entry></feed>