<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://s-sairam.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://s-sairam.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-03T00:48:34+00:00</updated><id>https://s-sairam.github.io/feed.xml</id><title type="html">blank</title><subtitle>Aspiring Causal &amp; Neuro-Symbolic AI Researcher</subtitle><entry><title type="html">Science, Not Sorcery: A First-Principles Replication of Sharpness-Aware Minimization</title><link href="https://s-sairam.github.io/blog/2025/sam-replication/" rel="alternate" type="text/html" title="Science, Not Sorcery: A First-Principles Replication of Sharpness-Aware Minimization"/><published>2025-11-03T00:00:00+00:00</published><updated>2025-11-03T00:00:00+00:00</updated><id>https://s-sairam.github.io/blog/2025/sam-replication</id><content type="html" xml:base="https://s-sairam.github.io/blog/2025/sam-replication/"><![CDATA[<h3 id="what-separates-a-good-model-from-a-robust-one">What separates a good model from a <em>robust</em> one?</h3> <p>In deep learning, it’s easy to achieve high accuracy on a training set. It’s far harder to build a model that generalizes reliably to the unseen chaos of the real world. Many state-of-the-art models are brittle, their success tied to a “sharp” minimum in the loss landscape—a narrow ravine that is easy to fall out of.</p> <p>The ICLR 2021 paper on <strong>Sharpness-Aware Minimization (SAM)</strong> proposed an elegant solution: force the optimizer to find not just a low point, but a wide, flat valley, making the model inherently more robust.</p> <p>This claim was powerful, and it led me to a critical question: could I, as an independent researcher with nothing but the paper itself, rebuild this complex system from scratch and verify its results? This project wasn’t about inventing a new method, but about engaging in a core, often-neglected, scientific practice: <strong>rigorous replication.</strong></p> <p>The goal was to perform a complete, clean-room implementation. Using only the paper’s algorithmic description, I engineered a PyTorch optimizer from first principles. It follows the paper’s two-step process: first, an adversarial ascent to find a point of higher loss, followed by a descent using the gradient from that perturbed position.</p> <p>The experiment was a success, reproducing the paper’s benchmark on CIFAR-10 with a Wide-ResNet-28-10.</p> <table> <thead> <tr> <th style="text-align: left">Experiment</th> <th style="text-align: center">Reported Test Accuracy (%)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>SAM (Foret et al., ICLR 2021)</strong></td> <td style="text-align: center"><strong>~97.3</strong></td> </tr> <tr> <td style="text-align: left"><strong>This First-Principles Replication</strong></td> <td style="text-align: center"><strong>~96.74</strong></td> </tr> </tbody> </table> <p><img src="/assets/img/train_loss.png" alt="W&amp;B Logs"/> <em>The full training run, publicly logged on Weights &amp; Biases, shows stable convergence to the final high-accuracy state.</em></p> <p>The final accuracy of <strong>96.74%</strong> is within <strong>0.56%</strong> of the original paper—a strong validation of the algorithm’s claims and the correctness of my implementation.</p> <h3 id="the-real-lesson-research-is-more-than-the-algorithm">The Real Lesson: Research is More Than the Algorithm</h3> <p>The most valuable outcome of this project was not the final accuracy number, but the insight gained from the tiny gap between my result and the paper’s. A clean, two-page algorithm in a paper belies the complex reality of its implementation. That 0.56% difference is a lesson in the hidden variables of deep learning research: minuscule differences in data augmentation pipelines, random seeds, software versions, or even GPU hardware can create subtle but measurable shifts in outcomes.</p> <p>This study taught me that true understanding of a research paper comes not from reading it, but from building it. It is in the building that you discover the unwritten details and develop an intuition for the system’s sensitivities.</p> <p>This commitment to deep, validated understanding is now the foundation of my current work. It directly motivates the development of <strong>[Artemis, my novel optimizer]</strong>, which combines the geometric insights I validated in SAM with principles of Bayesian uncertainty. This replication wasn’t just an exercise; it was the necessary groundwork for building the next generation of robust and reliable optimizers.</p> <p><a href="https://github.com/S-Sairam/sam-optimizer">View the Code on GitHub</a></p> <p><a href="https://wandb.ai/pesu-ai-ml/sam-replication-cifar10/runs/mjyz5xy4">See the Full, Transparent Training Logs on W&amp;B</a></p>]]></content><author><name></name></author><category term="Reproducibility"/><category term="Deep Learning"/><category term="Optimization"/><category term="Scientific Rigor"/><category term="Undergraduate Research"/><summary type="html"><![CDATA[An independent study in scientific reproducibility, rebuilding the ICLR 2021 SAM optimizer from scratch to validate its claims and understand its mechanics.]]></summary></entry><entry><title type="html">The Geometry of Thought: An Early Exploration in Structured Representation</title><link href="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/" rel="alternate" type="text/html" title="The Geometry of Thought: An Early Exploration in Structured Representation"/><published>2025-10-29T00:00:00+00:00</published><updated>2025-10-29T00:00:00+00:00</updated><id>https://s-sairam.github.io/blog/2025/the-geometry-of-thought</id><content type="html" xml:base="https://s-sairam.github.io/blog/2025/the-geometry-of-thought/"><![CDATA[<h3 id="can-we-force-a-model-to-think-in-a-more-structured-geometric-way">Can we force a model to think in a more structured, geometric way?</h3> <p>A standard deep learning model learns to represent data in a high-dimensional, unstructured latent space—a chaotic filing cabinet. It works, but it’s a black box. This raises a fundamental question: can we impose a deeper mathematical structure on a model’s internal “thoughts” to make them more interpretable and consistent?</p> <p>This question led to my first independent research project, undertaken out of my own interest in the intersection of pure mathematics and machine learning.</p> <p>Instead of looking to statistics for the answer, we turned to a deeper source of structure: <strong>algebraic number theory.</strong> We hypothesized that the relationships governing quadratic forms, as described by Manjul Bhargava’s work on integer cubes, could serve as a novel form of <strong>architectural regularization</strong> for a neural network.</p> <p>The goal was to build a system from first principles. We designed a differentiable loss function that operated independently of the main classification task. Its sole purpose was to penalize the model for creating mathematically inconsistent internal representations. We were testing if we could teach a machine not just <em>what</em> to think, but <em>how</em> its thoughts should be structured.</p> <p>The experiment yielded a clear result on the MNIST benchmark. While the model achieved a high accuracy of <strong>99.46%</strong>, the primary outcome was the structure of the latent space itself.</p> <p><img src="/assets/img/bhargava_cube_3d_visualization.png" alt="3D Embeddings"/></p> <p>As the visualization shows, the latent space is no longer an unstructured cloud. The embeddings have been organized into a distinct, clustered structure, a <strong>direct emergent consequence of the algebraic priors we imposed.</strong></p> <h3 id="the-key-lesson-from-novelty-to-rigor">The Key Lesson: From Novelty to Rigor</h3> <p>While this project achieved its proof-of-concept goals, its most valuable outcome was the lesson it taught me about the distinction between theoretical novelty and impactful research. It made me understand that a clever idea validated on a controlled, “toy” benchmark like MNIST is only the first step. True progress requires building systems that are not just elegant, but are also scalable and rigorously validated on complex, large-scale problems.</p> <p>This realization was the direct catalyst for my subsequent work. It now drives my research into <strong>[Artemis, a new class of optimizer]</strong> designed for superior performance on challenging, real-world tasks.</p> <p>This early study taught me that the goal isn’t just to build machines with beautiful internal structure, but to ensure that structure translates into verifiable gains in robustness and generalization where it matters most.</p> <p>To formalize this independent work, a paper was drafted and submitted for peer review. The process yielded invaluable feedback, highlighting several avenues for significant improvement—particularly the need for more rigorous, large-scale validation. Based on this expert critique, the paper was withdrawn to allow for the comprehensive rework it deserves.</p> <p><a href="https://github.com/S-Sairam/bcmem">View the Code on GitHub</a></p>]]></content><author><name></name></author><category term="Representation Learning"/><category term="Inductive Bias"/><category term="Architectural Regularization"/><category term="Undergraduate Research"/><summary type="html"><![CDATA[An independent undergraduate research project exploring the use of number-theoretic priors for structured representation learning.]]></summary></entry></feed>